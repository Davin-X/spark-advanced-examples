{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka Integration with PySpark\n",
    "\nPython equivalents of the Scala Kafka examples. This notebook demonstrates how to integrate Apache Kafka with PySpark for real-time data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "\nFirst, ensure you have the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this in terminal if needed)\n",
    "# pip install pyspark kafka-python\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "import time\n",
    "\n",
    "print(\"Dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark Session\n",
    "\nCreate a Spark session configured for Kafka integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with Kafka support\n",
    "spark = SparkSession.builder \\n",
    "    .appName(\"KafkaIntegration\") \\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(\"Spark session created with Kafka support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Producer - Writing to Kafka\n",
    "\nEquivalent to HelloProducer.scala - sending messages to Kafka topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data to send to Kafka\n",
    "sample_data = [\n",
    "    {\"id\": 1, \"name\": \"Alice\", \"age\": 25, \"city\": \"New York\"},\n",
    "    {\"id\": 2, \"name\": \"Bob\", \"age\": 30, \"city\": \"San Francisco\"},\n",
    "    {\"id\": 3, \"name\": \"Charlie\", \"age\": 35, \"city\": \"Chicago\"},\n",
    "    {\"id\": 4, \"name\": \"Diana\", \"age\": 28, \"city\": \"Boston\"}\n",
    "]\n",
    "\n",
    "# Create DataFrame from sample data\n",
    "df = spark.createDataFrame(sample_data)\n",
    "df.show()\n",
    "\n",
    "# Note: In production, you would write to Kafka like this:\n",
    "# df.selectExpr(\"CAST(id AS STRING) AS key\", \"to_json(struct(*)) AS value\") \\n",
    "#    .write \\n",
    "#    .format(\"kafka\") \\n",
    "#    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\n",
    "#    .option(\"topic\", \"user-data\") \\n",
    "#    .save()\n",
    "\n",
    "print(\"DataFrame ready for Kafka publishing (uncomment write code when Kafka is running)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Consumer - Reading from Kafka\n",
    "\nEquivalent to HelloConsumer.scala - consuming messages from Kafka topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Kafka (uncomment when Kafka is running)\n",
    "# kafka_df = spark \\n",
    "#     .readStream \\n",
    "#     .format(\"kafka\") \\n",
    "#     .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\n",
    "#     .option(\"subscribe\", \"user-data\") \\n",
    "#     .load()\n",
    "\n",
    "# # Parse JSON value column\n",
    "# parsed_df = kafka_df \\n",
    "#     .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")) \\n",
    "#     .select(\"data.*\")\n",
    "\n",
    "# # Display streaming data\n",
    "# query = parsed_df \\n",
    "#     .writeStream \\n",
    "#     .outputMode(\"append\") \\n",
    "#     .format(\"console\") \\n",
    "#     .start()\n",
    "\n",
    "# query.awaitTermination()\n",
    "\n",
    "print(\"Kafka consumer code ready (uncomment when Kafka is running)\")\n",
    "print(\"This demonstrates the PySpark Structured Streaming approach to Kafka consumption\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message Processing and Transformations\n",
    "\nProcess and transform the Kafka streaming data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for our JSON data\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Simulate processing pipeline (would run on streaming data)\n",
    "processed_df = df \\n",
    "    .withColumn(\"age_group\", \n",
    "                when(col(\"age\") < 30, \"Young\")\n",
    "                .when(col(\"age\") < 40, \"Middle-aged\")\n",
    "                .otherwise(\"Senior\")) \\n",
    "    .groupBy(\"city\", \"age_group\") \\n",
    "    .count() \\n",
    "    .orderBy(\"city\", \"age_group\")\n",
    "\n",
    "processed_df.show()\n",
    "\n",
    "print(\"Data processing pipeline ready for streaming data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling and Monitoring\n",
    "\nProduction-ready error handling patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error handling in streaming queries\n",
    "def process_batch(df, epoch_id):\n",
    "    try:\n",
    "        # Your processing logic here\n",
    "        result = df.groupBy(\"city\").count()\n",
    "        result.write.mode(\"append\").parquet(f\"/output/epoch_{epoch_id}\")\n",
    "        print(f\"Successfully processed batch {epoch_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {epoch_id}: {str(e)}\")\n",
    "        # Could write to dead letter queue or alert monitoring\n",
    "\n",
    "# Streaming query with error handling (uncomment when running)\n",
    "# query = df \\n",
    "#     .writeStream \\n",
    "#     .foreachBatch(process_batch) \\n",
    "#     .outputMode(\"update\") \\n",
    "#     .option(\"checkpointLocation\", \"/tmp/checkpoint\") \\n",
    "#     .start()\n",
    "\n",
    "# query.awaitTermination()\n",
    "\n",
    "print(\"Error handling patterns defined for production use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Differences: Scala vs Python\n",
    "\n### Scala Examples (connectivity/):\n",
    "- **HelloProducer.scala**: Direct Kafka client usage\n",
    "- **HelloConsumer.scala**: Low-level consumer implementation\n",
    "- **Focus**: Infrastructure-level integration\n",
    "\n### Python Notebooks (notebooks/):\n",
    "- **Structured Streaming**: High-level DataFrame API\n",
    "- **SQL Integration**: Declarative transformations\n",
    "- **Focus**: Data processing and analytics\n",
    "\n### When to Use Each:\n",
    "- **Scala**: Custom producers/consumers, performance-critical components\n",
    "- **Python**: Data analysis, ML pipelines, rapid prototyping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}