{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming Concepts\n",
    "\nInteractive exploration of streaming data processing with PySpark. This notebook covers the concepts demonstrated in the Scala streaming examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Types Overview\n",
    "\n### DStream (Discretized Stream)\n",
    "- **Basic streaming**: Socket, file, Kafka sources\n",
    "- **Micro-batch processing**: Fixed time intervals\n",
    "- **RDD-based**: Each batch is an RDD\n",
    "\n### Structured Streaming\n",
    "- **DataFrame/Dataset API**: Unified with batch processing\n",
    "- **Continuous processing**: Event-time processing\n",
    "- **SQL integration**: Declarative streaming queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "# Create Spark session for streaming\n",
    "spark = SparkSession.builder \\n",
    "    .appName(\"StreamingConcepts\") \\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/checkpoint\") \\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(\"Streaming session ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Socket Streaming (DStream)\n",
    "\nEquivalent to basic-streaming/streaming_*.scala examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Socket streaming requires a netcat server running\n",
    "# Terminal command: nc -lk 9999\n",
    "# Then send text data to see streaming in action\n",
    "\n",
    "# Socket streaming setup (uncomment to run)\n",
    "# from pyspark.streaming import StreamingContext\n",
    "\n",
    "# ssc = StreamingContext(spark.sparkContext, 5)  # 5 second batches\n",
    "\n",
    "# Create socket stream\n",
    "# lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Process each batch\n",
    "# words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "# word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Print results\n",
    "# word_counts.pprint()\n",
    "\n",
    "# Start streaming\n",
    "# ssc.start()\n",
    "# ssc.awaitTermination()\n",
    "\n",
    "print(\"Socket streaming code ready (requires netcat server on port 9999)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. File Streaming\n",
    "\nMonitor directory for new files - equivalent to file-streaming examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data directory\n",
    "import os\n",
    "os.makedirs(\"sample_data\", exist_ok=True)\n",
    "\n",
    "# Create sample files\n",
    "sample_texts = [\n",
    "    \"spark hadoop kafka streaming\",\n",
    "    \"machine learning data science\",\n",
    "    \"big data analytics pipeline\"\n",
    "]\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    with open(f\"sample_data/file_{i+1}.txt\", \"w\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "print(\"Sample files created in sample_data/ directory\")\n",
    "!ls -la sample_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File streaming setup (uncomment to run)\n",
    "# file_stream = spark \\n",
    "#     .readStream \\n",
    "#     .text(\"sample_data/\") \\n",
    "#     .withColumn(\"timestamp\", current_timestamp())\n",
    "\n",
    "# Process streaming files\n",
    "# word_counts = file_stream \\n",
    "#     .select(split(col(\"value\"), \" \").alias(\"words\")) \\n",
    "#     .select(explode(col(\"words\")).alias(\"word\"), \"timestamp\") \\n",
    "#     .groupBy(\"word\") \\n",
    "#     .count() \\n",
    "#     .orderBy(desc(\"count\"))\n",
    "\n",
    "# Display streaming results\n",
    "# query = word_counts \\n",
    "#     .writeStream \\n",
    "#     .outputMode(\"complete\") \\n",
    "#     .format(\"console\") \\n",
    "#     .trigger(processingTime=\"10 seconds\") \\n",
    "#     .start()\n",
    "\n",
    "# query.awaitTermination()\n",
    "\n",
    "print(\"File streaming code ready - monitors sample_data/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Structured Streaming\n",
    "\nAdvanced streaming with DataFrame API - equivalent to structured-streaming examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample structured data\n",
    "streaming_data = [\n",
    "    {\"event_id\": 1, \"user_id\": \"alice\", \"action\": \"login\", \"timestamp\": \"2023-01-01 10:00:00\"},\n",
    "    {\"event_id\": 2, \"user_id\": \"bob\", \"action\": \"view\", \"timestamp\": \"2023-01-01 10:01:00\"},\n",
    "    {\"event_id\": 3, \"user_id\": \"alice\", \"action\": \"purchase\", \"timestamp\": \"2023-01-01 10:02:00\"},\n",
    "    {\"event_id\": 4, \"user_id\": \"charlie\", \"action\": \"login\", \"timestamp\": \"2023-01-01 10:03:00\"}\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"event_id\", IntegerType()),\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"action\", StringType()),\n",
    "    StructField(\"timestamp\", StringType())\n",
    "])\n",
    "\n",
    "# Create static DataFrame for demonstration\n",
    "df = spark.createDataFrame(streaming_data, schema)\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "df.show()\n",
    "print(\"Structured data ready for streaming operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing Operations\n",
    "\nTime-based aggregations - equivalent to structured-streaming windowing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windowed aggregations (would run on streaming data)\n",
    "windowed_stats = df \\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\n",
    "    .groupBy(\n",
    "        window(\"timestamp\", \"5 minutes\"),\n",
    "        \"user_id\",\n",
    "        \"action\"\n",
    "    ) \\n",
    "    .count() \\n",
    "    .orderBy(\"window\", \"user_id\")\n",
    "\n",
    "windowed_stats.show(truncate=False)\n",
    "\n",
    "print(\"Windowing operations demonstrated on static data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Management\n",
    "\nMaintaining state across streaming batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User session tracking (would be used in streaming)\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Calculate session metrics\n",
    "session_window = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n",
    "\n",
    "user_sessions = df \\n",
    "    .withColumn(\"session_id\", \n",
    "                concat(col(\"user_id\"), \n",
    "                       lit(\"_\"),\n",
    "                       date_format(\"timestamp\", \"yyyyMMdd\"))) \\n",
    "    .withColumn(\"event_number\", row_number().over(session_window)) \\n",
    "    .withColumn(\"time_diff\", \n",
    "                unix_timestamp(\"timestamp\") - \n",
    "                lag(unix_timestamp(\"timestamp\")).over(session_window))\n",
    "\n",
    "user_sessions.show()\n",
    "\n",
    "print(\"State management concepts demonstrated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Best Practices\n",
    "\n### Checkpointing\n",
    "- **Purpose**: Fault tolerance and state recovery\n",
    "- **Location**: HDFS/S3/cloud storage\n",
    "- **Frequency**: Every few minutes\n",
    "\n### Watermarking\n",
    "- **Purpose**: Handle late-arriving data\n",
    "- **Configuration**: Based on expected delays\n",
    "- **Impact**: Affects memory usage\n",
    "\n### Trigger Intervals\n",
    "- **Default**: As fast as possible\n",
    "- **Production**: 1-10 minutes for cost optimization\n",
    "- **Real-time**: Millisecond triggers for latency-sensitive apps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scala vs Python Streaming\n",
    "\n### Scala Examples (streaming/):\n",
    "- **streaming_*.scala**: Low-level DStream operations\n",
    "- **Focus**: Infrastructure and performance\n",
    "- **Use case**: Custom streaming logic\n",
    "\n### Python Notebooks (notebooks/):\n",
    "- **Structured Streaming**: High-level DataFrame API\n",
    "- **Focus**: Data processing and analytics\n",
    "- **Use case**: ML pipelines, data transformations\n",
    "\n### Choosing the Right Approach:\n",
    "- **Scala**: Performance-critical streaming, custom receivers\n",
    "- **Python**: Data science workflows, ML integration, rapid development"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}